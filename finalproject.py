# -*- coding: utf-8 -*-
"""finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K3uLHjZxhokC2Y555F2BHjp35JFzNi4F
"""

!pip install kagglehub tensorflow pandas numpy matplotlib gradio transformers pydicom



import kagglehub
import pandas as pd
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import gradio as gr

print(tf.__version__)

import gradio as gr
print(gr.__version__)

print(np.__version__)



# Download the Indiana Chest X-ray dataset using Kaggle Hub
path = kagglehub.dataset_download("raddar/chest-xrays-indiana-university")
print("Dataset downloaded to:", path)

# Load the CSV files

projections = pd.read_csv(f"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv")  #/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv
reports = pd.read_csv(f"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv")

# Merge projections and reports by 'uid'
data = pd.merge(projections, reports, on="uid")
data.head()

import pandas as pd

csv_path = "/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv"
df = pd.read_csv(csv_path)

print("Columns in CSV:")
print(df.columns)



import pandas as pd

# Load CSVs
projections = pd.read_csv("/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv")
reports = pd.read_csv("/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv")

# Clean column names
projections.columns = projections.columns.str.strip().str.lower()
reports.columns = reports.columns.str.strip().str.lower()

# Check column names exist
assert "uid" in projections.columns and "uid" in reports.columns
assert "problems" in reports.columns

# Merge on 'uid'
df = pd.merge(projections, reports, on="uid")

# Drop rows with missing 'problems'
df = df.dropna(subset=["problems"])

import pandas as pd
import os

# Paths
image_dir = "/kaggle/input/chest-xrays-indiana-university/images/images_normalized"
projections_path = "/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv"
reports_path = "/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv"

# Load CSVs
projections = pd.read_csv(projections_path)
reports = pd.read_csv(reports_path)

# Merge on uid
df = pd.merge(projections, reports, on="uid")

# Create mapping from filename suffix to full path
image_files = os.listdir(image_dir)
suffix_to_path = {img.split("_", 1)[1]: os.path.join(image_dir, img) for img in image_files if "_" in img}

# Extract suffix from the 'filename' column
df["filename_suffix"] = df["filename"].apply(lambda x: x.split("_", 1)[1])

# Map suffix to actual path
df["image_path"] = df["filename_suffix"].map(suffix_to_path)

# Drop rows where mapping failed
df = df.dropna(subset=["image_path"])

print("âœ… Total usable images after matching:", len(df))
print("ðŸ©» Sample mapped image path:", df["image_path"].iloc[0])
print("ðŸ©º Sample label:", df['Problems'].iloc[0] if 'Problems' in df.columns else "No label column")

!pip install joblib

from sklearn.preprocessing import MultiLabelBinarizer

# Step 1: Clean and split multi-label text
df['Problems'] = df['Problems'].fillna('')
df['label_list'] = df['Problems'].apply(lambda x: x.lower().split('|'))  # Split by '|'

# Step 2: Binarize labels into multi-hot format
mlb = MultiLabelBinarizer()
label_matrix = mlb.fit_transform(df['label_list'])

# Optional: Save class names
class_labels = mlb.classes_
print(f"âœ… Total unique disease classes: {len(class_labels)}")

# Step 3: Train-test split
from sklearn.model_selection import train_test_split

image_paths = df['image_path'].values
y = label_matrix

train_paths, val_paths, y_train, y_val = train_test_split(image_paths, y, test_size=0.2, random_state=42)

print(f"âœ… Train shape: {y_train.shape}, Validation shape: {y_val.shape}")

num_classes = len(mlb.classes_)  # From MultiLabelBinarizer

import joblib
from google.colab import drive
drive.mount('/content/drive')

joblib.dump(mlb, '/content/drive/MyDrive/mlb_classes.pkl')
print(" mlb saved!")

import joblib

mlb = joblib.load('/content/drive/MyDrive/mlb_classes.pkl')
class_labels = mlb.classes_

print(f"ðŸ” Loaded mlbw with {len(class_labels)} classes")

print("Total number of X-ray images:", len(df))
print("Number of unique disease labels:", len(mlb.classes_))

import matplotlib.pyplot as plt
import seaborn as sns

label_counts = label_matrix.sum(axis=0)
sorted_counts = pd.Series(label_counts, index=mlb.classes_).sort_values(ascending=False)

plt.figure(figsize=(15, 6))
sns.barplot(x=sorted_counts.index[:30], y=sorted_counts.values[:30], palette="viridis")
plt.title("Top 30 Most Frequent Disease Labels in Indiana Chest X-ray Dataset")
plt.xlabel("Disease Labels")
plt.ylabel("Frequency")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

label_counts_per_image = label_matrix.sum(axis=1)

plt.figure(figsize=(8, 5))
sns.histplot(label_counts_per_image, bins=10, kde=True)
plt.title("Distribution of Number of Disease Labels Per Image")
plt.xlabel("Number of Labels")
plt.ylabel("Number of Images")
plt.grid(True)
plt.tight_layout()
plt.show()

projection_counts = df['projection'].value_counts()

plt.figure(figsize=(6, 4))
sns.barplot(x=projection_counts.index, y=projection_counts.values, palette="coolwarm")
plt.title("Distribution of X-ray Projection Types")
plt.xlabel("Projection Type")
plt.ylabel("Number of Images")
plt.tight_layout()
plt.show()

import cv2
import numpy as np

sample_paths = df['image_path'].sample(100).tolist()
shapes = [cv2.imread(p, cv2.IMREAD_GRAYSCALE).shape for p in sample_paths]

height_stats = [h for h, w in shapes]
width_stats = [w for h, w in shapes]

print("Average Image Height:", np.mean(height_stats))
print("Average Image Width:", np.mean(width_stats))

import matplotlib.pyplot as plt

sample = df.sample(5)
plt.figure(figsize=(15, 5))
for i, row in enumerate(sample.itertuples()):
    img = cv2.imread(row.image_path, cv2.IMREAD_GRAYSCALE)
    plt.subplot(1, 7, i + 1)
    plt.imshow(img, cmap='gray')
    plt.title("\n".join(row.label_list))
    plt.axis('off')
plt.tight_layout()
plt.show()

import numpy as np

corr_matrix = np.corrcoef(label_matrix.T)
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix[:15, :15], xticklabels=mlb.classes_[:15], yticklabels=mlb.classes_[:15], cmap="coolwarm", annot=True)
plt.title("Label Correlation Matrix (Top 15 Classes)")
plt.tight_layout()
plt.show()





import tensorflow as tf

IMG_SIZE = 256  # For consistency with your model's expected input size

def process_image(path, label):
    # Read and decode the image
    image = tf.io.read_file(path)
    image = tf.image.decode_png(image, channels=3)  # Decode as RGB (3 channels)

    # Resize image to match the expected input size for the model
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])  # Resize to 256x256

    # Normalize image to [0, 1] range
    image = tf.cast(image, tf.float32) / 255.0

    # Ensure label is in the correct data type (float32 for binary_crossentropy)
    label = tf.cast(label, tf.float32)

    return image, label

def create_dataset(image_paths, labels, batch_size=32):
    # Convert labels to tensors if they are not already
    labels = tf.convert_to_tensor(labels, dtype=tf.float32)

    # Create a TensorFlow dataset from image paths and labels
    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)
    label_ds = tf.data.Dataset.from_tensor_slices(labels)

    # Combine both datasets
    ds = tf.data.Dataset.zip((path_ds, label_ds))

    # Map the image processing function to each element of the dataset
    ds = ds.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)

    # Shuffle, batch, and prefetch the dataset for better performance
    ds = ds.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)

    return ds

# Assuming train_paths and y_train are lists of image file paths and their corresponding labels
# Create the training and validation datasets
train_dataset = create_dataset(train_paths, y_train)
val_dataset = create_dataset(val_paths, y_val)



import tensorflow as tf
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import AUC, Precision, Recall
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np

import tensorflow.keras.backend as K

def focal_loss(gamma=2., alpha=0.25):
    def loss_fn(y_true, y_pred):
        y_pred = K.clip(y_pred, 1e-7, 1 - 1e-7)
        pt = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)
        return -K.mean(alpha * K.pow(1. - pt, gamma) * K.log(pt))
    return loss_fn

from tensorflow.keras.applications import DenseNet121

base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
output = Dense(num_classes, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer=Adam(learning_rate=1e-4),
              loss=focal_loss(gamma=2., alpha=0.25),
              metrics=['accuracy', AUC(name='auc'), Precision(), Recall()])

early_stop = EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss')

history = model.fit(train_dataset,
                    validation_data=val_dataset,
                    epochs=10,
                    callbacks=[early_stop])

# Unfreeze some layers
base_model.trainable = True
for layer in base_model.layers[:100]:
    layer.trainable = False

model.compile(optimizer=Adam(learning_rate=1e-5),
              loss=focal_loss(gamma=2., alpha=0.25),
              metrics=['accuracy', AUC(name='auc'), Precision(), Recall()])

history_finetune = model.fit(train_dataset,
                             validation_data=val_dataset,
                             epochs=5,
                             callbacks=[early_stop])



for img, label in val_dataset.take(1):
    print("Label shape:", label.shape)
    print("Label vector (first sample):", label.numpy()[0])
    print("Sum of labels in this sample:", np.sum(label.numpy()[0]))

#correct one
from sklearn.metrics import precision_recall_curve, average_precision_score

# Initialize lists for plotting
precision_all = []
recall_all = []
ap_score_all = []

# For each class, compute PR curve and AP score
for i in range(y_true.shape[1]):
    precision, recall, _ = precision_recall_curve(y_true[:, i], y_probs[:, i])
    ap_score = average_precision_score(y_true[:, i], y_probs[:, i])

    precision_all.append(precision)
    recall_all.append(recall)
    ap_score_all.append(ap_score)

    # Optionally, plot the PR curve for each class
    plt.plot(recall, precision, label=f'Class {i} (AP={ap_score:.4f})')

# Plot overall PR curve
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curves for Each Class')
plt.legend()
plt.grid()
plt.show()

# Optionally print average AP score
average_ap_score = np.mean(ap_score_all)
print(f"Average AP Score (across all classes): {average_ap_score:.4f}")

custom_threshold = 0.2  # you can tune this between 0.1 to 0.5
y_pred = (y_probs > custom_threshold).astype(int)

from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred, zero_division=0, digits=4))

output = Dense(1430, activation='sigmoid')(x)

#model.save("densenet_xray_finetune_model3.keras")
model.save('/content/drive/MyDrive/densenet_xray_finetune_model3.h5')

from tensorflow.keras.models import load_model

def focal_loss(gamma=2., alpha=0.25):
    def loss_fn(y_true, y_pred):
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        return -tf.reduce_mean(alpha * tf.pow(1. - pt, gamma) * tf.math.log(pt))
    return loss_fn

model3 = load_model("/content/drive/MyDrive/densenet_xray_finetune_model3.h5", custom_objects={'loss_fn': focal_loss()})

num_classes = model3.output_shape[-1]
print(f"Number of disease classes in the model: {num_classes}")

import os
os.listdir('/content/drive/MyDrive')

from google.colab import files

# Open a file dialog to upload files
uploaded = files.upload()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate

# Step 1: Define U-Net architecture for RGB images
def unet(input_size=(256, 256, 3)):  # Changed input size to (256, 256, 3)
    inputs = Input(input_size)

    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)
    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)
    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)
    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)
    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)

    return Model(inputs=[inputs], outputs=[conv10])

# Step 2: Build the model
model = unet(input_size=(256, 256, 1))  # Ensuring it's built for RGB images
model.summary()
# Modify your U-Net input shape to (256, 256, 1) instead of (256, 256, 3)
#model = unet(input_size=(256, 256, 1))


# Step 3: Load pretrained weights (ensure you have the correct weights file)
model.load_weights('/content/cxr_reg_weights.best.hdf5')

# âœ… Step 4: Save the full model (architecture + weights)
model.save('/content/drive/MyDrive/unet_full_model.h5')

print("âœ… Full U-Net model (architecture + weights) saved successfully.")

from tensorflow.keras.models import load_model
unet_model = load_model('/content/drive/MyDrive/unet_full_model.h5')

!pip install gradio

#proper code correct one
import gradio as gr
import tensorflow as tf
import numpy as np
from PIL import Image
import joblib
from tensorflow.keras.models import load_model
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# Load models and data
unet_model = load_model('/content/drive/MyDrive/unet_full_model.h5', compile=False)
resnet_model = load_model("/content/drive/MyDrive/densenet_xray_finetune_model3.h5", custom_objects={'loss_fn': focal_loss()})
mlb = joblib.load('/content/drive/MyDrive/mlb_classes.pkl')
class_labels = mlb.classes_
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2")

IMG_SIZE = 256
THRESHOLD = 0.2

# --- Lung Segmentation ---
def predict_mask(image):
    image = image.convert("L").resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(image) / 255.0
    img_array = np.expand_dims(img_array, axis=(0, -1))  # Shape: (1, 256, 256, 1)
    pred_mask = unet_model.predict(img_array)[0, :, :, 0]
    pred_mask = (pred_mask > 0.5).astype(np.uint8) * 255
    return Image.fromarray(pred_mask)

# --- Generate Explanation using GPT-2 ---
def generate_explanation(disease):
    prompt = f"The patient shows signs of {disease}. The medical explanation is:"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output_ids = gpt2_model.generate(
        input_ids,
        max_length=100,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        early_stopping=True
    )
    explanation = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return explanation.replace(prompt, "").strip()

# --- Disease Prediction (with Explanation) ---
def predict_diseases(image):
    image = image.resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(image) / 255.0
    if img_array.shape[-1] == 4:
        img_array = img_array[..., :3]
    img_array = np.expand_dims(img_array, axis=0)

    preds = resnet_model.predict(img_array)[0]
    pred_indices = np.where(preds >= THRESHOLD)[0]

    if len(pred_indices) == 0:
        return "No disease confidently detected.", ""

    result = {class_labels[i]: float(preds[i]) for i in pred_indices}
    sorted_result = dict(sorted(result.items(), key=lambda x: x[1], reverse=True))

    top_disease = list(sorted_result.keys())[0]
    explanation = generate_explanation(top_disease)

    return sorted_result, f"**{top_disease}**: {explanation}"

# --- Top 3 Disease Prediction (No Explanation) ---
def predict_top3_diseases(image):
    image = image.resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(image) / 255.0
    if img_array.shape[-1] == 4:
        img_array = img_array[..., :3]
    img_array = np.expand_dims(img_array, axis=0)

    preds = resnet_model.predict(img_array)[0]

    # Get top 3 predictions regardless of threshold
    top_3_indices = np.argsort(preds)[-3:][::-1]
    top_3 = {class_labels[i]: round(float(preds[i]), 3) for i in top_3_indices}

    return top_3

# --- Gradio Interfaces for Tabs ---
segmentation_tab = gr.Interface(
    fn=predict_mask,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=gr.Image(type="pil", label="Predicted Lung Mask"),
    title="Lung Segmentation using U-Net",
    description="Upload a chest X-ray to view the lung segmentation mask."
)

prediction_tab = gr.Interface(
    fn=predict_diseases,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=[
        gr.Label(num_top_classes=10, label="Predicted Diseases"),
        gr.Textbox(label="AI-Generated Explanation for Top Disease")
    ],
    title="Disease Prediction using ResNet50 + GPT-2",
    description="Upload a chest X-ray to get predicted diseases and an AI-generated explanation."
)

top3_tab = gr.Interface(
    fn=predict_top3_diseases,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=gr.Label(num_top_classes=3, label="Top 3 Predicted Diseases"),
    title="Top 3 Disease Predictions",
    description="Upload a chest X-ray to get the top 3 predicted diseases (no explanation)."
)

# --- Launch with 3 Tabs ---
gr.TabbedInterface(
    interface_list=[segmentation_tab, prediction_tab, top3_tab],
    tab_names=["Lung Segmentation", "Disease Prediction", "Top 3 Diseases"]
).launch()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate

print(tf.__version__)

!pip install reportlab

import gradio as gr
import tensorflow as tf
import numpy as np
from PIL import Image
import joblib
from tensorflow.keras.models import load_model
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
import tempfile
import os

# Load models and data
def focal_loss(gamma=2., alpha=.25):
    def loss_fn(y_true, y_pred):
        epsilon = 1e-9
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        cross_entropy = -y_true * tf.math.log(y_pred)
        loss = alpha * tf.math.pow(1 - y_pred, gamma) * cross_entropy
        return tf.reduce_mean(loss)
    return loss_fn

unet_model = load_model('/content/drive/MyDrive/unet_full_model.h5', compile=False)
resnet_model = load_model("/content/drive/MyDrive/densenet_xray_finetune_model3.h5", custom_objects={'loss_fn': focal_loss()})
mlb = joblib.load('/content/drive/MyDrive/mlb_classes.pkl')
class_labels = mlb.classes_
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2")

IMG_SIZE = 256
THRESHOLD = 0.2

# --- Lung Segmentation ---
def predict_mask(image):
    image = image.convert("L").resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(image) / 255.0
    img_array = np.expand_dims(img_array, axis=(0, -1))
    pred_mask = unet_model.predict(img_array)[0, :, :, 0]
    pred_mask = (pred_mask > 0.5).astype(np.uint8) * 255
    return Image.fromarray(pred_mask)

# --- GPT-2 Explanation ---
def generate_explanation(disease):
    prompt = f"The patient shows signs of {disease}. The medical explanation is:"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output_ids = gpt2_model.generate(
        input_ids,
        max_length=100,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        early_stopping=True
    )
    explanation = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return explanation.replace(prompt, "").strip()

# --- PDF Generator for Top 3 Tab ---
def generate_top3_pdf(segmentation_img, top3_dict, explanation_text):
    pdf_path = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf").name
    c = canvas.Canvas(pdf_path, pagesize=letter)
    width, height = letter

    c.setFont("Helvetica-Bold", 16)
    c.drawString(50, height - 50, "Chest X-ray Top 3 Disease Report")

    # Predicted top 3 diseases
    c.setFont("Helvetica", 12)
    y = height - 100
    c.drawString(50, y, "Top 3 Predicted Diseases and Probabilities:")
    y -= 20
    for disease, prob in top3_dict.items():
        c.drawString(70, y, f"{disease}: {prob:.3f}")
        y -= 20

    # Explanation
    if explanation_text:
        c.setFont("Helvetica-Bold", 12)
        c.drawString(50, y - 10, "AI Explanation for Top Disease:")
        y -= 30
        c.setFont("Helvetica", 11)
        for line in explanation_text.split('\n'):
            c.drawString(70, y, line.strip())
            y -= 20

    c.showPage()

    # Add segmentation image
    if segmentation_img:
        seg_path = tempfile.NamedTemporaryFile(delete=False, suffix=".png").name
        segmentation_img.save(seg_path)
        c.drawImage(seg_path, 50, 300, width=400, preserveAspectRatio=True)
        c.setFont("Helvetica-Bold", 12)
        c.drawString(50, 270, "Segmented Lung Mask")
        c.save()
        os.remove(seg_path)
    else:
        c.save()

    return pdf_path

# --- Top 3 Prediction + PDF ---
def predict_top3_with_pdf(image):
    resized_img = image.resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(resized_img) / 255.0
    if img_array.shape[-1] == 4:
        img_array = img_array[..., :3]
    img_array = np.expand_dims(img_array, axis=0)

    preds = resnet_model.predict(img_array)[0]
    top_3_indices = np.argsort(preds)[-3:][::-1]
    top3 = {class_labels[i]: round(float(preds[i]), 3) for i in top_3_indices}
    top_disease = list(top3.keys())[0]
    explanation = generate_explanation(top_disease)
    segmentation = predict_mask(image)

    pdf_path = generate_top3_pdf(segmentation, top3, f"{top_disease}: {explanation}")
    return top3, pdf_path

# --- Original Disease Prediction Tab (with Explanation + PDF) ---
def predict_diseases(image):
    image = image.resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(image) / 255.0
    if img_array.shape[-1] == 4:
        img_array = img_array[..., :3]
    img_array = np.expand_dims(img_array, axis=0)

    preds = resnet_model.predict(img_array)[0]
    pred_indices = np.where(preds >= THRESHOLD)[0]

    if len(pred_indices) == 0:
        return "No disease confidently detected.", "", None

    result = {class_labels[i]: float(preds[i]) for i in pred_indices}
    sorted_result = dict(sorted(result.items(), key=lambda x: x[1], reverse=True))

    top_disease = list(sorted_result.keys())[0]
    explanation = generate_explanation(top_disease)
    pdf_path = generate_top3_pdf(None, sorted_result, f"{top_disease}: {explanation}")

    return sorted_result, f"**{top_disease}**: {explanation}", pdf_path

# --- Segmentation Tab ---
segmentation_tab = gr.Interface(
    fn=predict_mask,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=gr.Image(type="pil", label="Predicted Lung Mask"),
    title="Lung Segmentation using U-Net",
    description="Upload a chest X-ray to view the lung segmentation mask."
)

# --- Prediction Tab ---
prediction_tab = gr.Interface(
    fn=predict_diseases,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=[
        gr.Label(num_top_classes=10, label="Predicted Diseases"),
        gr.Textbox(label="AI-Generated Explanation for Top Disease"),
        gr.File(label="Download PDF Report")
    ],
    title="Disease Prediction using ResNet50 + GPT-2",
    description="Upload a chest X-ray to get predicted diseases, an AI-generated explanation, and download a report."
)

# --- Top 3 Tab with PDF ---
top3_tab = gr.Interface(
    fn=predict_top3_with_pdf,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=[
        gr.Label(num_top_classes=3, label="Top 3 Predicted Diseases"),
        gr.File(label="Download PDF Report (Top 3 + Segmentation)")
    ],
    title="Top 3 Disease Predictions + PDF Report",
    description="Upload a chest X-ray to get the top 3 predicted diseases, lung segmentation, and a downloadable PDF report."
)

# --- Launch Tabs ---
gr.TabbedInterface(
    interface_list=[segmentation_tab, prediction_tab, top3_tab],
    tab_names=["Lung Segmentation", "Disease Prediction", "Top 3 Diseases"]
).launch()

import gradio as gr
import tensorflow as tf
import numpy as np
from PIL import Image
import joblib
from tensorflow.keras.models import load_model
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
import tempfile
import os

# Load models and data
def focal_loss(gamma=2., alpha=.25):
    def loss_fn(y_true, y_pred):
        epsilon = 1e-9
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        cross_entropy = -y_true * tf.math.log(y_pred)
        loss = alpha * tf.math.pow(1 - y_pred, gamma) * cross_entropy
        return tf.reduce_mean(loss)
    return loss_fn

unet_model = load_model('/content/drive/MyDrive/unet_full_model.h5', compile=False)
resnet_model = load_model("/content/drive/MyDrive/densenet_xray_finetune_model3.h5", custom_objects={'loss_fn': focal_loss()})
mlb = joblib.load('/content/drive/MyDrive/mlb_classes.pkl')
class_labels = mlb.classes_
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2")

IMG_SIZE = 256
THRESHOLD = 0.2

# --- Lung Segmentation ---
def predict_mask(image):
    image = image.convert("L").resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(image) / 255.0
    img_array = np.expand_dims(img_array, axis=(0, -1))
    pred_mask = unet_model.predict(img_array)[0, :, :, 0]
    pred_mask = (pred_mask > 0.5).astype(np.uint8) * 255
    return Image.fromarray(pred_mask)

# --- GPT-2 Explanation ---
def generate_explanation(disease):
    prompt = f"The patient shows signs of {disease}. The medical explanation is:"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output_ids = gpt2_model.generate(
        input_ids,
        max_length=100,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        early_stopping=True
    )
    explanation = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return explanation.replace(prompt, "").strip()

# --- PDF Generator ---
def generate_top3_pdf(segmentation_img, top3_dict, explanation_text):
    pdf_path = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf").name
    c = canvas.Canvas(pdf_path, pagesize=letter)
    width, height = letter

    c.setFont("Helvetica-Bold", 16)
    c.drawString(50, height - 50, "Chest X-ray Top 3 Disease Report")

    # Top 3 predictions
    c.setFont("Helvetica", 12)
    y = height - 100
    c.drawString(50, y, "Top 3 Predicted Diseases and Probabilities:")
    y -= 20
    for disease, prob in top3_dict.items():
        c.drawString(70, y, f"{disease}: {prob:.3f}")
        y -= 20

    # Explanation
    if explanation_text:
        c.setFont("Helvetica-Bold", 12)
        c.drawString(50, y - 10, "AI Explanation for Top Disease:")
        y -= 30
        c.setFont("Helvetica", 11)
        for line in explanation_text.split('\n'):
            c.drawString(70, y, line.strip())
            y -= 20

    c.showPage()

    # Segmented image
    if segmentation_img:
        seg_path = tempfile.NamedTemporaryFile(delete=False, suffix=".png").name
        segmentation_img.save(seg_path)
        c.drawImage(seg_path, 50, 300, width=400, preserveAspectRatio=True)
        c.setFont("Helvetica-Bold", 12)
        c.drawString(50, 270, "Segmented Lung Mask")
        c.save()
        os.remove(seg_path)
    else:
        c.save()

    return pdf_path

# --- Top 3 + PDF ---
def predict_top3_with_pdf(image):
    resized_img = image.resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(resized_img) / 255.0
    if img_array.shape[-1] == 4:
        img_array = img_array[..., :3]
    img_array = np.expand_dims(img_array, axis=0)

    preds = resnet_model.predict(img_array)[0]
    top_3_indices = np.argsort(preds)[-3:][::-1]
    top3 = {class_labels[i]: round(float(preds[i]), 3) for i in top_3_indices}
    top_disease = list(top3.keys())[0]
    explanation = generate_explanation(top_disease)
    segmentation = predict_mask(image)

    pdf_path = generate_top3_pdf(segmentation, top3, f"{top_disease}: {explanation}")
    return top3, pdf_path

# --- Original Prediction Tab (No PDF) ---
def predict_diseases(image):
    image = image.resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(image) / 255.0
    if img_array.shape[-1] == 4:
        img_array = img_array[..., :3]
    img_array = np.expand_dims(img_array, axis=0)

    preds = resnet_model.predict(img_array)[0]
    pred_indices = np.where(preds >= THRESHOLD)[0]

    if len(pred_indices) == 0:
        return "No disease confidently detected.", ""

    result = {class_labels[i]: float(preds[i]) for i in pred_indices}
    sorted_result = dict(sorted(result.items(), key=lambda x: x[1], reverse=True))
    top_disease = list(sorted_result.keys())[0]
    explanation = generate_explanation(top_disease)
    return sorted_result, f"**{top_disease}**: {explanation}"

# --- Segmentation Tab ---
segmentation_tab = gr.Interface(
    fn=predict_mask,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=gr.Image(type="pil", label="Predicted Lung Mask"),
    title="Lung Segmentation using U-Net",
    description="Upload a chest X-ray to view the lung segmentation mask."
)

# --- Prediction Tab (No PDF) ---
prediction_tab = gr.Interface(
    fn=predict_diseases,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=[
        gr.Label(num_top_classes=10, label="Predicted Diseases"),
        gr.Textbox(label="AI-Generated Explanation for Top Disease")
    ],
    title="Disease Prediction using ResNet50 + GPT-2",
    description="Upload a chest X-ray to get predicted diseases and an AI-generated explanation."
)

# --- Top 3 Tab with PDF ---
top3_tab = gr.Interface(
    fn=predict_top3_with_pdf,
    inputs=gr.Image(type="pil", label="Upload Chest X-ray"),
    outputs=[
        gr.Label(num_top_classes=3, label="Top 3 Predicted Diseases"),
        gr.File(label="Download PDF Report (Top 3 + Segmentation)")
    ],
    title="Top 3 Disease Predictions + PDF Report",
    description="Upload a chest X-ray to get the top 3 predicted diseases, lung segmentation, and a downloadable PDF report."
)

# --- Launch App ---
gr.TabbedInterface(
    interface_list=[segmentation_tab, prediction_tab, top3_tab],
    tab_names=["Lung Segmentation", "Disease Prediction", "Top 3 Diseases"]
).launch()

import reportlab
print(reportlab.__version__)

